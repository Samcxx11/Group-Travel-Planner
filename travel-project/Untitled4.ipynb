{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "meSrxoy4CgDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa22816-1c3e-408f-b468-a3cfc194a810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m327.7/981.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m364.3/364.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "üì• Downloading NLTK data...\n",
            "‚úÖ All dependencies installed successfully!\n",
            "‚úÖ NLTK data downloaded!\n",
            "‚úÖ Eventlet installed for SocketIO!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install flask flask-cors flask-sqlalchemy flask-socketio python-dotenv nltk pandas pyngrok langdetect textblob deep-translator eventlet -q\n",
        "\n",
        "import nltk\n",
        "print(\"üì• Downloading NLTK data...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('brown', quiet=True)\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n",
        "print(\"‚úÖ NLTK data downloaded!\")\n",
        "print(\"‚úÖ Eventlet installed for SocketIO!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tOT2ms5GDi_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301149f5-c2b7-4e27-8840-707081f20f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n",
            "‚úÖ Using deep-translator (no httpx conflicts!)\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from flask_sqlalchemy import SQLAlchemy\n",
        "from flask_socketio import SocketIO, emit, join_room\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from deep_translator import GoogleTranslator\n",
        "from langdetect import detect, DetectorFactory\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# NLTK imports\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"‚úÖ Using deep-translator (no httpx conflicts!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BSU6YM7aEane",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184cac83-a2ae-498e-b329-4821aed5aa67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Flask app initialized (REST API mode)!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "app = Flask(__name__)\n",
        "app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///travel_planner.db'\n",
        "app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n",
        "app.config['SECRET_KEY'] = 'your-secret-key-for-colab'\n",
        "\n",
        "db = SQLAlchemy(app)\n",
        "\n",
        "CORS(app, resources={\n",
        "    r\"/*\": {\n",
        "        \"origins\": \"*\",\n",
        "        \"methods\": [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"],\n",
        "        \"allow_headers\": [\"Content-Type\", \"Authorization\", \"ngrok-skip-browser-warning\"],\n",
        "        \"expose_headers\": [\"Content-Type\"],\n",
        "        \"supports_credentials\": False\n",
        "    }\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "    response.headers.add('Access-Control-Allow-Origin', '*')\n",
        "    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,ngrok-skip-browser-warning')\n",
        "    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Flask app initialized (REST API mode)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2s1nnSjrEiHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ba73ba-136f-4f9e-cd33-f9c26c752a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Database models defined!\n"
          ]
        }
      ],
      "source": [
        "class User(db.Model):\n",
        "    __tablename__ = 'users'\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    username = db.Column(db.String(100), unique=True, nullable=False)\n",
        "    email = db.Column(db.String(120), unique=True, nullable=False)\n",
        "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
        "\n",
        "class Preference(db.Model):\n",
        "    __tablename__ = 'preferences'\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)\n",
        "    group_id = db.Column(db.Integer, db.ForeignKey('groups.id'))\n",
        "    preference_text = db.Column(db.Text, nullable=False)\n",
        "    original_language = db.Column(db.String(10))\n",
        "    translated_text = db.Column(db.Text)\n",
        "    extracted_categories = db.Column(db.JSON)\n",
        "    extracted_keywords = db.Column(db.JSON)\n",
        "    sentiment_score = db.Column(db.Float)\n",
        "    budget = db.Column(db.String(20))\n",
        "    duration_days = db.Column(db.Integer)\n",
        "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
        "\n",
        "class Group(db.Model):\n",
        "    __tablename__ = 'groups'\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    name = db.Column(db.String(200), nullable=False)\n",
        "    created_by = db.Column(db.Integer, db.ForeignKey('users.id'))\n",
        "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
        "\n",
        "class GroupMember(db.Model):\n",
        "    __tablename__ = 'group_members'\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    group_id = db.Column(db.Integer, db.ForeignKey('groups.id'), nullable=False)\n",
        "    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)\n",
        "    joined_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
        "\n",
        "class Attraction(db.Model):\n",
        "    __tablename__ = 'attractions'\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    name = db.Column(db.String(200), nullable=False)\n",
        "    location = db.Column(db.String(200))\n",
        "    state = db.Column(db.String(100))\n",
        "    category = db.Column(db.String(50))\n",
        "    description = db.Column(db.Text)\n",
        "    rating = db.Column(db.Float)\n",
        "    estimated_time_hours = db.Column(db.Float)\n",
        "    best_time_of_day = db.Column(db.String(20))\n",
        "    entry_fee = db.Column(db.Float)\n",
        "\n",
        "class Itinerary(db.Model):\n",
        "    __tablename__ = 'itineraries'\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    group_id = db.Column(db.Integer, db.ForeignKey('groups.id'), nullable=False)\n",
        "    name = db.Column(db.String(200))\n",
        "    total_days = db.Column(db.Integer)\n",
        "    generated_plan = db.Column(db.JSON)\n",
        "    status = db.Column(db.String(20), default='draft')\n",
        "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
        "\n",
        "class Expense(db.Model):\n",
        "    __tablename__ = 'expenses'\n",
        "    id = db.Column(db.Integer, primary_key=True)\n",
        "    group_id = db.Column(db.Integer, db.ForeignKey('groups.id'), nullable=False)\n",
        "    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)\n",
        "    description = db.Column(db.String(300), nullable=False)\n",
        "    amount = db.Column(db.Float, nullable=False)\n",
        "    category = db.Column(db.String(50), nullable=False)\n",
        "    paid_by = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)\n",
        "    split_among = db.Column(db.JSON)\n",
        "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
        "\n",
        "print(\"‚úÖ Database models defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gkFPPYDIEvsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f916223-b981-4ae0-cac7-8bdfd6bfdc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Enhanced NLP with NLTK initialized!\n",
            "‚úÖ Smart Itinerary Generator initialized!\n",
            "‚úÖ Using deep-translator (no conflicts!)\n",
            "üìö NLP Features: Tokenization, POS Tagging, Lemmatization, Sentiment Analysis\n",
            "üìç Smart routing: Geographically sensible itineraries\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class EnhancedLanguageProcessor:\n",
        "    def __init__(self):\n",
        "        self.translator = GoogleTranslator()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        self.supported_languages = {\n",
        "            'en': 'English',\n",
        "            'hi': 'Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä)',\n",
        "            'bn': 'Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)',\n",
        "            'ta': 'Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)',\n",
        "            'te': 'Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)',\n",
        "            'ml': 'Malayalam (‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç)',\n",
        "            'mr': 'Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)',\n",
        "            'gu': 'Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)',\n",
        "            'kn': 'Kannada (‡≤ï‡≤®‡≥ç‡≤®‡≤°)',\n",
        "            'pa': 'Punjabi (‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä)',\n",
        "            'or': 'Odia (‡¨ì‡¨°‡¨º‡¨ø‡¨Ü)',\n",
        "            'as': 'Assamese (‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ)',\n",
        "            'ur': 'Urdu (ÿßÿ±ÿØŸà)'\n",
        "        }\n",
        "\n",
        "        self.english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        \"\"\"Detect language using langdetect\"\"\"\n",
        "        try:\n",
        "            detected = detect(text)\n",
        "            lang_name = self.supported_languages.get(detected, f'Unknown ({detected})')\n",
        "            print(f\"üåê Language detected: {detected} - {lang_name}\")\n",
        "            return detected\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Detection failed: {e}, defaulting to English\")\n",
        "            return 'en'\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"Tokenize using NLTK - fallback to simple split if punkt fails\"\"\"\n",
        "        try:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            print(f\"üìù NLTK Tokens (first 15): {tokens[:15]}\")\n",
        "            return tokens\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è NLTK tokenization failed, using simple split\")\n",
        "            tokens = text.lower().split()\n",
        "            print(f\"üìù Simple Tokens (first 15): {tokens[:15]}\")\n",
        "            return tokens\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Remove stopwords using NLTK\"\"\"\n",
        "        filtered = [word for word in tokens if word.isalnum() and word not in self.english_stopwords]\n",
        "        print(f\"üßπ Stopword removal: {len(tokens)} ‚Üí {len(filtered)} tokens\")\n",
        "        return filtered\n",
        "\n",
        "    def lemmatize_tokens(self, tokens):\n",
        "        \"\"\"Lemmatize tokens using NLTK WordNet\"\"\"\n",
        "        lemmatized = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "        print(f\"üî§ Lemmatized (first 10): {lemmatized[:10]}\")\n",
        "        return lemmatized\n",
        "\n",
        "    def extract_pos_tags(self, text):\n",
        "        \"\"\"Extract POS tags and categorize words - with fallback\"\"\"\n",
        "        try:\n",
        "            tokens = word_tokenize(text)\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è Using simple tokenization for POS tagging\")\n",
        "            tokens = text.split()\n",
        "\n",
        "        try:\n",
        "            pos_tags = pos_tag(tokens)\n",
        "\n",
        "            nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
        "            verbs = [word for word, pos in pos_tags if pos.startswith('VB')]\n",
        "            adjectives = [word for word, pos in pos_tags if pos.startswith('JJ')]\n",
        "\n",
        "            print(f\"üè∑Ô∏è POS Analysis:\")\n",
        "            print(f\"   Nouns: {nouns[:8]}\")\n",
        "            print(f\"   Verbs: {verbs[:5]}\")\n",
        "            print(f\"   Adjectives: {adjectives[:5]}\")\n",
        "\n",
        "            return {\n",
        "                'nouns': nouns,\n",
        "                'verbs': verbs,\n",
        "                'adjectives': adjectives,\n",
        "                'all_tags': pos_tags\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è POS tagging error: {e}\")\n",
        "            return {\n",
        "                'nouns': [],\n",
        "                'verbs': [],\n",
        "                'adjectives': [],\n",
        "                'all_tags': []\n",
        "            }\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
        "        try:\n",
        "            blob = TextBlob(text)\n",
        "            sentiment = blob.sentiment.polarity\n",
        "\n",
        "            if sentiment > 0.1:\n",
        "                sentiment_label = \"Positive üòä\"\n",
        "            elif sentiment < -0.1:\n",
        "                sentiment_label = \"Negative üòü\"\n",
        "            else:\n",
        "                sentiment_label = \"Neutral üòê\"\n",
        "\n",
        "            print(f\"üòä Sentiment: {sentiment_label} (score: {sentiment:.2f})\")\n",
        "            return sentiment\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def translate_to_english(self, text, source_lang):\n",
        "        \"\"\"Translate to English using Google Translate\"\"\"\n",
        "        if source_lang == 'en':\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            translated = GoogleTranslator(source=source_lang, target='en').translate(text)\n",
        "            print(f\"üîÑ Translation:\")\n",
        "            print(f\"   Original ({source_lang}): {text}\")\n",
        "            print(f\"   English: {translated}\")\n",
        "            return translated\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Translation error: {e}\")\n",
        "            return text\n",
        "\n",
        "\n",
        "class EnhancedPreferenceExtractor:\n",
        "    def __init__(self):\n",
        "        self.category_keywords = {\n",
        "            'religious': [\n",
        "                'temple', 'church', 'mosque', 'gurudwara', 'shrine', 'monastery',\n",
        "                'spiritual', 'pilgrimage', 'worship', 'deity', 'god', 'goddess', 'prayer',\n",
        "                '‡§Æ‡§Ç‡§¶‡§ø‡§∞', '‡§Æ‡§∏‡•ç‡§ú‡§ø‡§¶', '‡§ö‡§∞‡•ç‡§ö', '‡¶ó‡¶ø‡¶∞‡ßç‡¶ú‡¶æ', '‡¶Æ‡¶∏‡¶ú‡¶ø‡¶¶', '‡Æï‡Øã‡ÆØ‡Æø‡Æ≤‡Øç', '‡∞¶‡±á‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç',\n",
        "                '‡¥ï‡µç‡¥∑‡µá‡¥§‡µç‡¥∞‡¥Ç', '‡≤¶‡≥á‡≤µ‡≤æ‡≤≤‡≤Ø', '‡™Æ‡™Ç‡™¶‡™ø‡™∞', '‡®ó‡©Å‡®∞‡®¶‡©Å‡®Ü‡®∞‡®æ', '‡¨Æ‡¨®‡≠ç‡¨¶‡¨ø‡¨∞'\n",
        "            ],\n",
        "            'beach': [\n",
        "                'beach', 'sea', 'ocean', 'coast', 'shore', 'island', 'sand', 'wave', 'surf',\n",
        "                '‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞', '‡§§‡§ü', '‡¶∏‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞', '‡¶∏‡ßà‡¶ï‡¶§', '‡Æï‡Æü‡Æ±‡Øç‡Æï‡Æ∞‡Øà', '‡∞∏‡∞Æ‡±Å‡∞¶‡±ç‡∞∞‡∞Ç', '‡¥ï‡¥ü‡µΩ', '‡≤∏‡≤Æ‡≥Å‡≤¶‡≥ç‡≤∞',\n",
        "                '‡™∏‡™Æ‡´Å‡™¶‡´ç‡™∞', '‡®∏‡®Æ‡©Å‡©∞‡®¶‡®∞', '‡¨∏‡¨Æ‡≠Å‡¨¶‡≠ç‡¨∞'\n",
        "            ],\n",
        "            'historical': [\n",
        "                'historical', 'monument', 'fort', 'palace', 'heritage', 'ancient',\n",
        "                'ruins', 'museum', 'castle', 'archaeological', 'historic',\n",
        "                '‡§ï‡§ø‡§≤‡§æ', '‡§Æ‡§π‡§≤', '‡§ê‡§§‡§ø‡§π‡§æ‡§∏‡§ø‡§ï', '‡¶ï‡¶ø‡¶≤‡ßç‡¶≤‡¶æ', '‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶æ‡¶¶', '‡Æï‡Øã‡Æü‡Øç‡Æü‡Øà', '‡∞ï‡±ã‡∞ü',\n",
        "                '‡¥ï‡µã‡¥ü‡µç‡¥ü', '‡≤ï‡≥ã‡≤ü‡≥Ü', '‡™ï‡™ø‡™≤‡´ç‡™≤‡´ã', '‡®ï‡®ø‡®≤‡©ç‡®π‡®æ', '‡¨¶‡≠Å‡¨∞‡≠ç‡¨ó'\n",
        "            ],\n",
        "            'nature': [\n",
        "                'nature', 'park', 'wildlife', 'forest', 'mountain', 'hill', 'valley',\n",
        "                'waterfall', 'lake', 'garden', 'trek', 'hiking', 'camping', 'natural',\n",
        "                '‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø', '‡§™‡§æ‡§∞‡•ç‡§ï', '‡¶ú‡¶ô‡ßç‡¶ó‡¶≤', '‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§‡¶ø', '‡Æá‡ÆØ‡Æ±‡Øç‡Æï‡Øà', '‡∞™‡±ç‡∞∞‡∞ï‡±É‡∞§‡∞ø',\n",
        "                '‡¥™‡µç‡¥∞‡¥ï‡µÉ‡¥§‡¥ø', '‡≤™‡≥ç‡≤∞‡≤ï‡≥É‡≤§‡≤ø', '‡™™‡´ç‡™∞‡™ï‡´É‡™§‡™ø', '‡®ï‡©Å‡®¶‡®∞‡®§', '‡¨™‡≠ç‡¨∞‡¨ï‡≠É‡¨§‡¨ø'\n",
        "            ],\n",
        "            'adventure': [\n",
        "                'adventure', 'trekking', 'rafting', 'paragliding', 'camping',\n",
        "                'rock climbing', 'skiing', 'diving', 'bungee', 'zip line', 'safari',\n",
        "                '‡§∏‡§æ‡§π‡§∏‡§ø‡§ï', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡ßá‡¶û‡ßç‡¶ö‡¶æ‡¶∞', '‡Æö‡Ææ‡Æï‡Æö‡ÆÆ‡Øç', '‡∞∏‡∞æ‡∞π‡∞∏‡∞Ç', '‡¥∏‡¥æ‡¥π‡¥∏‡¥ø‡¥ï‡¥§',\n",
        "                '‡≤∏‡≤æ‡≤π‡≤∏', '‡™∏‡™æ‡™π‡™∏', '‡®ê‡®°‡®µ‡©à‡®Ç‡®ö‡®∞'\n",
        "            ],\n",
        "            'cultural': [\n",
        "                'cultural', 'art', 'dance', 'music', 'festival', 'local',\n",
        "                'traditional', 'handicraft', 'folk', 'culture',\n",
        "                '‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï', '‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø', '‡¶∏‡¶æ‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø‡¶ï', '‡Æï‡Æ≤‡Ææ‡Æö‡Ææ‡Æ∞‡ÆÆ‡Øç', '‡∞∏‡∞æ‡∞Ç‡∞∏‡±ç‡∞ï‡±É‡∞§‡∞ø‡∞ï',\n",
        "                '‡¥∏‡¥Ç‡¥∏‡µç‡¥ï‡¥æ‡¥∞‡¥Ç', '‡≤∏‡≤Ç‡≤∏‡≥ç‡≤ï‡≥É‡≤§‡≤ø', '‡™∏‡™Ç‡™∏‡´ç‡™ï‡´É‡™§‡™ø', '‡®∏‡©±‡®≠‡®ø‡®Ü‡®ö‡®æ‡®∞'\n",
        "            ],\n",
        "            'food': [\n",
        "                'food', 'cuisine', 'restaurant', 'street food', 'culinary', 'dish',\n",
        "                'meal', 'eating', 'dining', 'taste', 'delicious',\n",
        "                '‡§ñ‡§æ‡§®‡§æ', '‡§≠‡•ã‡§ú‡§®', '‡¶ñ‡¶æ‡¶¨‡¶æ‡¶∞', '‡Æâ‡Æ£‡Æµ‡ØÅ', '‡∞Ü‡∞π‡∞æ‡∞∞‡∞Ç', '‡¥≠‡¥ï‡µç‡¥∑‡¥£‡¥Ç', '‡≤Ü‡≤π‡≤æ‡≤∞',\n",
        "                '‡™ñ‡´ã‡™∞‡™æ‡™ï', '‡®≠‡©ã‡®ú‡®®', '‡¨ñ‡¨æ‡¨¶‡≠ç‡≠ü'\n",
        "            ],\n",
        "            'shopping': [\n",
        "                'shopping', 'market', 'bazaar', 'mall', 'shop', 'buy', 'purchase',\n",
        "                '‡§¨‡§æ‡§ú‡§æ‡§∞', '‡§ñ‡§∞‡•Ä‡§¶‡§æ‡§∞‡•Ä', '‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞', '‡Æö‡Æ®‡Øç‡Æ§‡Øà', '‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±Ü‡∞ü‡±ç', '‡¥∑‡µã‡¥™‡µç‡¥™‡¥ø‡¥Ç‡¥ó‡µç',\n",
        "                '‡≤∂‡≤æ‡≤™‡≤ø‡≤Ç‡≤ó‡≥ç', '‡™¨‡™ú‡™æ‡™∞', '‡®¨‡®æ‡®ú‡®º‡®æ‡®∞'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        self.budget_keywords = {\n",
        "            'low': [\n",
        "                'budget', 'cheap', 'affordable', 'economical', 'inexpensive', 'low cost',\n",
        "                '‡§∏‡§∏‡•ç‡§§‡§æ', '‡¶ï‡¶Æ ‡¶¶‡¶æ‡¶Æ', '‡ÆÆ‡Æ≤‡Æø‡Æµ‡ØÅ', '‡∞ö‡±å‡∞ï‡±à‡∞®', '‡¥µ‡¥ø‡¥≤‡¥ï‡µÅ‡¥±‡¥û‡µç‡¥û', '‡≤Ö‡≤ó‡≥ç‡≤ó‡≤¶', '‡™∏‡™∏‡´ç‡™§‡´Å‡™Ç', '‡®∏‡®∏‡®§‡®æ'\n",
        "            ],\n",
        "            'medium': [\n",
        "                'moderate', 'medium', 'reasonable', 'average', 'mid range',\n",
        "                '‡§Æ‡§ß‡•ç‡§Ø‡§Æ', '‡¶Æ‡¶æ‡¶ù‡¶æ‡¶∞‡¶ø', '‡Æ®‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æ∞', '‡∞Æ‡∞ß‡±ç‡∞Ø‡∞∏‡±ç‡∞•', '‡¥Æ‡¥¶‡µç‡¥ß‡µç‡¥Ø‡¥Ç', '‡≤Æ‡≤ß‡≥ç‡≤Ø‡≤Æ', '‡™Æ‡™ß‡´ç‡™Ø‡™Æ', '‡®Æ‡©±‡®ß‡®Æ'\n",
        "            ],\n",
        "            'high': [\n",
        "                'luxury', 'premium', 'expensive', 'high end', 'upscale', 'lavish',\n",
        "                '‡§Æ‡§π‡§Ç‡§ó‡§æ', '‡¶¶‡¶æ‡¶Æ‡¶ø', '‡Æµ‡Æø‡Æ≤‡Øà ‡Æâ‡ÆØ‡Æ∞‡Øç‡Æ®‡Øç‡Æ§', '‡∞ñ‡∞∞‡±Ä‡∞¶‡±à‡∞®', '‡¥µ‡¥ø‡¥≤‡¥Ø‡µá‡¥±‡¥ø‡¥Ø', '‡≤¶‡≥Å‡≤¨‡≤æ‡≤∞‡≤ø', '‡™Æ‡´ã‡™Ç‡™ò‡´Å‡™Ç', '‡®Æ‡®π‡®ø‡©∞‡®ó‡®æ'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def extract_categories(self, text, tokens):\n",
        "        \"\"\"Extract categories from text and tokens\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        found_categories = set()\n",
        "\n",
        "        for category, keywords in self.category_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in text_lower:\n",
        "                    found_categories.add(category)\n",
        "                    break\n",
        "\n",
        "        for category, keywords in self.category_keywords.items():\n",
        "            for token in tokens:\n",
        "                if token in keywords or any(kw in token for kw in keywords if len(kw) > 3):\n",
        "                    found_categories.add(category)\n",
        "                    break\n",
        "\n",
        "        result = list(found_categories) if found_categories else ['general']\n",
        "        print(f\"üè∑Ô∏è Categories extracted: {result}\")\n",
        "        return result\n",
        "\n",
        "    def extract_budget(self, text):\n",
        "        \"\"\"Extract budget level\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for budget_level, keywords in self.budget_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in text_lower:\n",
        "                    print(f\"üí∞ Budget detected (keyword): {budget_level}\")\n",
        "                    return budget_level\n",
        "\n",
        "        numbers = re.findall(r'\\d+', text)\n",
        "        if numbers:\n",
        "            budget_amount = int(numbers[0])\n",
        "            if budget_amount < 10000:\n",
        "                budget = 'low'\n",
        "            elif budget_amount < 50000:\n",
        "                budget = 'medium'\n",
        "            else:\n",
        "                budget = 'high'\n",
        "            print(f\"üí∞ Budget detected (amount): {budget} (‚Çπ{budget_amount})\")\n",
        "            return budget\n",
        "\n",
        "        print(\"üí∞ Budget defaulted: medium\")\n",
        "        return 'medium'\n",
        "\n",
        "    def extract_duration(self, text):\n",
        "        \"\"\"Extract trip duration\"\"\"\n",
        "        patterns = [\n",
        "            r'(\\d+)\\s*days?', r'(\\d+)\\s*day', r'(\\d+)\\s*‡§¶‡§ø‡§®', r'(\\d+)\\s*‡¶¶‡¶ø‡¶®',\n",
        "            r'(\\d+)\\s*‡Æ®‡Ææ‡Æ≥‡Øç', r'(\\d+)\\s*‡∞∞‡±ã‡∞ú‡±Å‡∞≤‡±Å', r'(\\d+)\\s*‡¥¶‡¥ø‡¥µ‡¥∏‡¥Ç', r'(\\d+)\\s*‡≤¶‡≤ø‡≤®',\n",
        "            r'(\\d+)\\s*‡™¶‡™ø‡™µ‡™∏', r'(\\d+)\\s*‡®¶‡®ø‡®®', r'(\\d+)\\s*‡¨¶‡¨ø‡¨®'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text.lower())\n",
        "            if match:\n",
        "                duration = int(match.group(1))\n",
        "                print(f\"üìÖ Duration extracted: {duration} days\")\n",
        "                return duration\n",
        "\n",
        "        print(\"üìÖ Duration defaulted: 3 days\")\n",
        "        return 3\n",
        "\n",
        "    def extract_keywords(self, tokens, pos_tags):\n",
        "        \"\"\"Extract important keywords\"\"\"\n",
        "        important = []\n",
        "        for word, tag in pos_tags:\n",
        "            if (tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ')) and len(word) > 3:\n",
        "                important.append(word.lower())\n",
        "\n",
        "        keywords = list(set(important))[:15]\n",
        "        print(f\"üîë Keywords extracted: {keywords}\")\n",
        "        return keywords\n",
        "\n",
        "\n",
        "class ItineraryAggregator:\n",
        "    def aggregate_preferences(self, preferences):\n",
        "        all_categories = []\n",
        "        budgets = []\n",
        "        durations = []\n",
        "\n",
        "        for pref in preferences:\n",
        "            if pref.extracted_categories:\n",
        "                all_categories.extend(pref.extracted_categories)\n",
        "            if pref.budget:\n",
        "                budgets.append(pref.budget)\n",
        "            if pref.duration_days:\n",
        "                durations.append(pref.duration_days)\n",
        "\n",
        "        category_counts = Counter(all_categories)\n",
        "        budget_mode = Counter(budgets).most_common(1)[0][0] if budgets else 'medium'\n",
        "        avg_duration = sum(durations) // len(durations) if durations else 3\n",
        "\n",
        "        return {\n",
        "            'categories': dict(category_counts),\n",
        "            'budget': budget_mode,\n",
        "            'duration': avg_duration\n",
        "        }\n",
        "\n",
        "    def generate_itinerary(self, aggregated, attractions):\n",
        "        duration = aggregated['duration']\n",
        "        categories = aggregated['categories']\n",
        "        sorted_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        itinerary = []\n",
        "        used_attractions = set()\n",
        "\n",
        "        for day in range(1, duration + 1):\n",
        "            day_plan = {\n",
        "                'day': day,\n",
        "                'activities': {'morning': None, 'afternoon': None, 'evening': None}\n",
        "            }\n",
        "\n",
        "            for time_slot in ['morning', 'afternoon', 'evening']:\n",
        "                if sorted_categories:\n",
        "                    category_idx = ((day - 1) * 3 + ['morning', 'afternoon', 'evening'].index(time_slot)) % len(sorted_categories)\n",
        "                    target_category = sorted_categories[category_idx][0]\n",
        "\n",
        "                    suitable_attractions = [\n",
        "                        a for a in attractions\n",
        "                        if a.category == target_category and a.id not in used_attractions\n",
        "                    ]\n",
        "\n",
        "                    if suitable_attractions:\n",
        "                        selected = suitable_attractions[0]\n",
        "                        day_plan['activities'][time_slot] = {\n",
        "                            'attraction_id': selected.id,\n",
        "                            'name': selected.name,\n",
        "                            'location': selected.location,\n",
        "                            'category': selected.category,\n",
        "                            'estimated_time': selected.estimated_time_hours\n",
        "                        }\n",
        "                        used_attractions.add(selected.id)\n",
        "\n",
        "            itinerary.append(day_plan)\n",
        "\n",
        "        return itinerary\n",
        "\n",
        "\n",
        "class SmartItineraryGenerator:\n",
        "    \"\"\"\n",
        "    Intelligent itinerary generator that creates geographically sensible plans\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Group Indian attractions by region for travel feasibility\n",
        "        self.regions = {\n",
        "            'North': ['Delhi', 'Agra', 'Jaipur', 'Amritsar', 'Katra', 'Rishikesh', 'Nainital', 'Chamoli', 'Bir'],\n",
        "            'South': ['Madurai', 'Hampi', 'Kerala', 'Alleppey', 'Mysore', 'Chennai', 'Kottayam'],\n",
        "            'West': ['Goa', 'South Goa', 'North Goa', 'Old Goa', 'Mumbai'],\n",
        "            'East': ['Kolkata'],\n",
        "            'Islands': ['Havelock Island', 'Andaman'],\n",
        "            'Rajasthan': ['Jaipur', 'Rajasthan']\n",
        "        }\n",
        "\n",
        "    def get_region(self, location):\n",
        "        \"\"\"Identify which region an attraction belongs to\"\"\"\n",
        "        for region, locations in self.regions.items():\n",
        "            for loc in locations:\n",
        "                if loc.lower() in location.lower():\n",
        "                    return region\n",
        "        return 'Other'\n",
        "\n",
        "    def generate_smart_itinerary(self, aggregated, attractions):\n",
        "        \"\"\"\n",
        "        Generate a geographically sensible itinerary\n",
        "        \"\"\"\n",
        "        duration = aggregated['duration']\n",
        "        categories = aggregated['categories']\n",
        "\n",
        "        # Sort categories by user preference\n",
        "        sorted_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)\n",
        "        preferred_categories = [cat[0] for cat in sorted_categories]\n",
        "\n",
        "        # Filter attractions by preferred categories\n",
        "        relevant_attractions = [\n",
        "            a for a in attractions\n",
        "            if a.category in preferred_categories\n",
        "        ]\n",
        "\n",
        "        # Sort by rating (highest first)\n",
        "        relevant_attractions.sort(key=lambda x: x.rating if x.rating else 0, reverse=True)\n",
        "\n",
        "        # Group attractions by region\n",
        "        attractions_by_region = {}\n",
        "        for attr in relevant_attractions:\n",
        "            region = self.get_region(attr.location)\n",
        "            if region not in attractions_by_region:\n",
        "                attractions_by_region[region] = []\n",
        "            attractions_by_region[region].append(attr)\n",
        "\n",
        "        # Select the best region\n",
        "        if not attractions_by_region:\n",
        "            return self._generate_fallback_itinerary(duration)\n",
        "\n",
        "        # Pick region with most attractions\n",
        "        best_region = max(attractions_by_region.items(), key=lambda x: len(x[1]))[0]\n",
        "        region_attractions = attractions_by_region[best_region]\n",
        "\n",
        "        # Generate day-by-day plan\n",
        "        itinerary = []\n",
        "        used_attractions = set()\n",
        "\n",
        "        for day in range(1, duration + 1):\n",
        "            day_plan = {\n",
        "                'day': day,\n",
        "                'region': best_region,\n",
        "                'activities': {\n",
        "                    'morning': None,\n",
        "                    'afternoon': None,\n",
        "                    'evening': None\n",
        "                }\n",
        "            }\n",
        "\n",
        "            time_slots = ['morning', 'afternoon', 'evening']\n",
        "\n",
        "            for slot in time_slots:\n",
        "                for attr in region_attractions:\n",
        "                    if attr.id not in used_attractions:\n",
        "                        if attr.best_time_of_day == slot or attr.best_time_of_day is None:\n",
        "                            day_plan['activities'][slot] = {\n",
        "                                'attraction_id': attr.id,\n",
        "                                'name': attr.name,\n",
        "                                'location': attr.location,\n",
        "                                'state': attr.state,\n",
        "                                'category': attr.category,\n",
        "                                'rating': attr.rating,\n",
        "                                'estimated_time': attr.estimated_time_hours,\n",
        "                                'entry_fee': attr.entry_fee,\n",
        "                                'description': attr.description\n",
        "                            }\n",
        "                            used_attractions.add(attr.id)\n",
        "                            break\n",
        "\n",
        "                if day_plan['activities'][slot] is None:\n",
        "                    for attr in region_attractions:\n",
        "                        if attr.id not in used_attractions:\n",
        "                            day_plan['activities'][slot] = {\n",
        "                                'attraction_id': attr.id,\n",
        "                                'name': attr.name,\n",
        "                                'location': attr.location,\n",
        "                                'state': attr.state,\n",
        "                                'category': attr.category,\n",
        "                                'rating': attr.rating,\n",
        "                                'estimated_time': attr.estimated_time_hours,\n",
        "                                'entry_fee': attr.entry_fee,\n",
        "                                'description': attr.description\n",
        "                            }\n",
        "                            used_attractions.add(attr.id)\n",
        "                            break\n",
        "\n",
        "            itinerary.append(day_plan)\n",
        "\n",
        "        return itinerary\n",
        "\n",
        "    def _generate_fallback_itinerary(self, duration):\n",
        "        \"\"\"Fallback for when no attractions match\"\"\"\n",
        "        itinerary = []\n",
        "        for day in range(1, duration + 1):\n",
        "            itinerary.append({\n",
        "                'day': day,\n",
        "                'region': 'General',\n",
        "                'activities': {\n",
        "                    'morning': None,\n",
        "                    'afternoon': None,\n",
        "                    'evening': None\n",
        "                }\n",
        "            })\n",
        "        return itinerary\n",
        "\n",
        "\n",
        "# Initialize NLP components\n",
        "lang_processor = EnhancedLanguageProcessor()\n",
        "pref_extractor = EnhancedPreferenceExtractor()\n",
        "aggregator = ItineraryAggregator()\n",
        "smart_generator = SmartItineraryGenerator()\n",
        "\n",
        "print(\"‚úÖ Enhanced NLP with NLTK initialized!\")\n",
        "print(\"‚úÖ Smart Itinerary Generator initialized!\")\n",
        "print(\"‚úÖ Using deep-translator (no conflicts!)\")\n",
        "print(\"üìö NLP Features: Tokenization, POS Tagging, Lemmatization, Sentiment Analysis\")\n",
        "print(\"üìç Smart routing: Geographically sensible itineraries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_kX2E9miE4Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb8e648-2995-413a-e482-b1c806df36bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API routes defined with NLTK processing!\n"
          ]
        }
      ],
      "source": [
        "# Check if routes are already registered, if so skip all route definitions\n",
        "if 'home' in app.view_functions:\n",
        "    print(\"‚ö†Ô∏è Routes already registered. Skipping...\")\n",
        "else:\n",
        "    @app.route('/')\n",
        "    def home():\n",
        "        return jsonify({\n",
        "            'message': 'Enhanced Travel Planner with NLTK',\n",
        "            'status': 'success',\n",
        "            'nlp_features': [\n",
        "                'NLTK Tokenization',\n",
        "                'POS Tagging (Nouns, Verbs, Adjectives)',\n",
        "                'Lemmatization',\n",
        "                'Stopword Removal',\n",
        "                'Sentiment Analysis',\n",
        "                'Multi-language Support (13 Indian languages)'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    @app.route('/api/languages', methods=['GET'])\n",
        "    def get_supported_languages():\n",
        "        return jsonify({\n",
        "            'languages': lang_processor.supported_languages,\n",
        "            'count': len(lang_processor.supported_languages)\n",
        "        })\n",
        "\n",
        "    @app.route('/api/users', methods=['POST'])\n",
        "    def create_user():\n",
        "        try:\n",
        "            data = request.json\n",
        "            existing_user = User.query.filter_by(email=data['email']).first()\n",
        "            if existing_user:\n",
        "                return jsonify({\n",
        "                    'id': existing_user.id,\n",
        "                    'username': existing_user.username,\n",
        "                    'message': 'User logged in!'\n",
        "                }), 200\n",
        "\n",
        "            user = User(username=data['username'], email=data['email'])\n",
        "            db.session.add(user)\n",
        "            db.session.commit()\n",
        "            return jsonify({\n",
        "                'id': user.id,\n",
        "                'username': user.username,\n",
        "                'message': 'User created!'\n",
        "            }), 201\n",
        "        except Exception as e:\n",
        "            db.session.rollback()\n",
        "            return jsonify({'error': str(e)}), 400\n",
        "\n",
        "    @app.route('/api/preferences', methods=['POST'])\n",
        "    def submit_preference():\n",
        "        try:\n",
        "            data = request.json\n",
        "            original_text = data['preference_text']\n",
        "\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"üîç NLTK-POWERED NLP PROCESSING\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "            # Step 1: Detect language\n",
        "            detected_lang = lang_processor.detect_language(original_text)\n",
        "\n",
        "            # Step 2: Translate to English\n",
        "            english_text = lang_processor.translate_to_english(original_text, detected_lang)\n",
        "\n",
        "            # Step 3: Tokenize\n",
        "            tokens = lang_processor.tokenize_text(english_text)\n",
        "\n",
        "            # Step 4: Remove stopwords\n",
        "            filtered_tokens = lang_processor.remove_stopwords(tokens)\n",
        "\n",
        "            # Step 5: Lemmatize\n",
        "            lemmatized = lang_processor.lemmatize_tokens(filtered_tokens)\n",
        "\n",
        "            # Step 6: POS tagging\n",
        "            pos_info = lang_processor.extract_pos_tags(english_text)\n",
        "\n",
        "            # Step 7: Sentiment analysis\n",
        "            sentiment = lang_processor.analyze_sentiment(english_text)\n",
        "\n",
        "            # Step 8: Extract categories\n",
        "            categories = pref_extractor.extract_categories(english_text, lemmatized)\n",
        "\n",
        "            # Step 9: Extract budget\n",
        "            budget = pref_extractor.extract_budget(english_text)\n",
        "\n",
        "            # Step 10: Extract duration\n",
        "            duration = pref_extractor.extract_duration(english_text)\n",
        "\n",
        "            # Step 11: Extract keywords\n",
        "            keywords = pref_extractor.extract_keywords(lemmatized, pos_info['all_tags'])\n",
        "\n",
        "            print(\"=\"*70)\n",
        "            print(\"‚úÖ NLP PROCESSING COMPLETE\")\n",
        "            print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "            # Save preference\n",
        "            preference = Preference(\n",
        "                user_id=data['user_id'],\n",
        "                group_id=data.get('group_id'),\n",
        "                preference_text=original_text,\n",
        "                original_language=detected_lang,\n",
        "                translated_text=english_text,\n",
        "                extracted_categories=categories,\n",
        "                extracted_keywords=keywords,\n",
        "                sentiment_score=sentiment,\n",
        "                budget=budget,\n",
        "                duration_days=duration\n",
        "            )\n",
        "            db.session.add(preference)\n",
        "            db.session.commit()\n",
        "\n",
        "            return jsonify({\n",
        "                'id': preference.id,\n",
        "                'original_language': detected_lang,\n",
        "                'language_name': lang_processor.supported_languages.get(detected_lang, 'Unknown'),\n",
        "                'original_text': original_text,\n",
        "                'translated_text': english_text,\n",
        "                'categories': categories,\n",
        "                'keywords': keywords,\n",
        "                'nouns_found': pos_info['nouns'][:5],\n",
        "                'verbs_found': pos_info['verbs'][:5],\n",
        "                'adjectives_found': pos_info['adjectives'][:5],\n",
        "                'sentiment': sentiment,\n",
        "                'budget': budget,\n",
        "                'duration': duration,\n",
        "                'nlp_engine': 'NLTK + TextBlob'\n",
        "            }), 201\n",
        "        except Exception as e:\n",
        "            db.session.rollback()\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return jsonify({'error': str(e)}), 400\n",
        "\n",
        "    @app.route('/api/groups', methods=['POST'])\n",
        "    def create_group():\n",
        "        try:\n",
        "            data = request.json\n",
        "            group = Group(name=data['name'], created_by=data['user_id'])\n",
        "            db.session.add(group)\n",
        "            db.session.commit()\n",
        "\n",
        "            member = GroupMember(group_id=group.id, user_id=data['user_id'])\n",
        "            db.session.add(member)\n",
        "            db.session.commit()\n",
        "\n",
        "            return jsonify({'id': group.id, 'name': group.name}), 201\n",
        "        except Exception as e:\n",
        "            db.session.rollback()\n",
        "            return jsonify({'error': str(e)}), 400\n",
        "\n",
        "    @app.route('/api/groups/<int:group_id>/itinerary', methods=['POST'])\n",
        "    def generate_itinerary(group_id):\n",
        "        try:\n",
        "            preferences = Preference.query.filter_by(group_id=group_id).all()\n",
        "\n",
        "            if not preferences:\n",
        "                return jsonify({'error': 'No preferences found for this group'}), 400\n",
        "\n",
        "            # Aggregate preferences\n",
        "            aggregated = aggregator.aggregate_preferences(preferences)\n",
        "\n",
        "            # Get all attractions\n",
        "            attractions = Attraction.query.all()\n",
        "\n",
        "            # ‚úÖ CHANGED: Use smart generator instead of basic one\n",
        "            itinerary_plan = smart_generator.generate_smart_itinerary(aggregated, attractions)\n",
        "\n",
        "            # Calculate trip summary\n",
        "            total_cost = 0\n",
        "            attractions_count = 0\n",
        "\n",
        "            for day in itinerary_plan:\n",
        "                for slot in ['morning', 'afternoon', 'evening']:\n",
        "                    activity = day['activities'][slot]\n",
        "                    if activity:\n",
        "                        total_cost += activity.get('entry_fee', 0)\n",
        "                        attractions_count += 1\n",
        "\n",
        "            # Save itinerary\n",
        "            itinerary = Itinerary(\n",
        "                group_id=group_id,\n",
        "                name=f\"{itinerary_plan[0]['region'] if itinerary_plan else 'India'} Trip - {aggregated['duration']} days\",\n",
        "                total_days=aggregated['duration'],\n",
        "                generated_plan=itinerary_plan\n",
        "            )\n",
        "            db.session.add(itinerary)\n",
        "            db.session.commit()\n",
        "\n",
        "            return jsonify({\n",
        "                'id': itinerary.id,\n",
        "                'plan': itinerary_plan,\n",
        "                'summary': {\n",
        "                    'region': itinerary_plan[0]['region'] if itinerary_plan else 'General',\n",
        "                    'total_days': aggregated['duration'],\n",
        "                    'total_attractions': attractions_count,\n",
        "                    'estimated_entry_fees': total_cost,\n",
        "                    'budget_level': aggregated['budget']\n",
        "                }\n",
        "            }), 201\n",
        "\n",
        "        except Exception as e:\n",
        "            db.session.rollback()\n",
        "            print(f\"‚ùå Error generating itinerary: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return jsonify({'error': str(e)}), 400\n",
        "\n",
        "    @app.route('/api/itineraries/<int:itinerary_id>', methods=['GET'])\n",
        "    def get_itinerary(itinerary_id):\n",
        "        try:\n",
        "            itinerary = Itinerary.query.get_or_404(itinerary_id)\n",
        "            return jsonify({\n",
        "                'id': itinerary.id,\n",
        "                'name': itinerary.name,\n",
        "                'total_days': itinerary.total_days,\n",
        "                'generated_plan': itinerary.generated_plan,\n",
        "                'status': itinerary.status\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return jsonify({'error': str(e)}), 400\n",
        "\n",
        "    @app.route('/api/attractions', methods=['GET'])\n",
        "    def get_attractions():\n",
        "        try:\n",
        "            attractions = Attraction.query.all()\n",
        "            return jsonify([{\n",
        "                'id': a.id, 'name': a.name, 'location': a.location,\n",
        "                'category': a.category, 'rating': a.rating\n",
        "            } for a in attractions])\n",
        "        except Exception as e:\n",
        "            return jsonify({'error': str(e)}), 400\n",
        "\n",
        "    @app.route('/api/groups/<int:group_id>', methods=['GET'])\n",
        "    def get_group(group_id):\n",
        "        try:\n",
        "            group = Group.query.get_or_404(group_id)\n",
        "            members = db.session.query(User).join(GroupMember).filter(GroupMember.group_id == group_id).all()\n",
        "\n",
        "            return jsonify({\n",
        "                'id': group.id,\n",
        "                'name': group.name,\n",
        "                'members': [{'id': m.id, 'username': m.username} for m in members]\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return jsonify({'error': str(e)}), 400\n",
        "\n",
        "    print(\"‚úÖ API routes defined with NLTK processing!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "irWmB7pxFfM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86a35de-893a-4af3-d68b-d8a85e16897b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Database created!\n",
            "üìã Preference columns: ['id', 'user_id', 'group_id', 'preference_text', 'original_language', 'translated_text', 'extracted_categories', 'extracted_keywords', 'sentiment_score', 'budget', 'duration_days', 'created_at']\n",
            "‚úÖ New schema confirmed with sentiment_score!\n",
            "‚úÖ Seeded 17 attractions!\n",
            "\n",
            "‚úÖ Database ready!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Force delete old database\n",
        "db_path = 'travel_planner.db'\n",
        "if os.path.exists(db_path):\n",
        "    try:\n",
        "        db.session.remove()\n",
        "        db.engine.dispose()\n",
        "    except:\n",
        "        pass\n",
        "    os.remove(db_path)\n",
        "    print(f\"üóëÔ∏è Deleted old database\")\n",
        "\n",
        "# Create new database\n",
        "with app.app_context():\n",
        "    db.create_all()\n",
        "    print(\"‚úÖ Database created!\")\n",
        "\n",
        "    # Verify schema\n",
        "    from sqlalchemy import inspect\n",
        "    inspector = inspect(db.engine)\n",
        "    columns = [col['name'] for col in inspector.get_columns('preferences')]\n",
        "    print(f\"üìã Preference columns: {columns}\")\n",
        "\n",
        "    if 'sentiment_score' in columns:\n",
        "        print(\"‚úÖ New schema confirmed with sentiment_score!\")\n",
        "\n",
        "    # Seed attractions\n",
        "    if Attraction.query.count() == 0:\n",
        "        attractions_data = [\n",
        "            # Religious Sites\n",
        "            {'name': 'Golden Temple', 'location': 'Amritsar', 'state': 'Punjab', 'category': 'religious',\n",
        "             'description': 'Holiest Gurudwara of Sikhs', 'rating': 4.9, 'estimated_time_hours': 3.0,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 0},\n",
        "            {'name': 'Meenakshi Temple', 'location': 'Madurai', 'state': 'Tamil Nadu', 'category': 'religious',\n",
        "             'description': 'Historic Hindu temple', 'rating': 4.8, 'estimated_time_hours': 2.5,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 50},\n",
        "            {'name': 'Vaishno Devi', 'location': 'Katra', 'state': 'J&K', 'category': 'religious',\n",
        "             'description': 'Famous pilgrimage', 'rating': 4.8, 'estimated_time_hours': 6.0,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 0},\n",
        "\n",
        "            # Beaches\n",
        "            {'name': 'Radhanagar Beach', 'location': 'Havelock Island', 'state': 'Andaman', 'category': 'beach',\n",
        "             'description': 'Pristine beach', 'rating': 4.7, 'estimated_time_hours': 4.0,\n",
        "             'best_time_of_day': 'afternoon', 'entry_fee': 0},\n",
        "            {'name': 'Palolem Beach', 'location': 'South Goa', 'state': 'Goa', 'category': 'beach',\n",
        "             'description': 'Scenic beach', 'rating': 4.6, 'estimated_time_hours': 3.0,\n",
        "             'best_time_of_day': 'evening', 'entry_fee': 0},\n",
        "            {'name': 'Kovalam Beach', 'location': 'Kerala', 'state': 'Kerala', 'category': 'beach',\n",
        "             'description': 'Lighthouse beach', 'rating': 4.5, 'estimated_time_hours': 3.0,\n",
        "             'best_time_of_day': 'afternoon', 'entry_fee': 0},\n",
        "\n",
        "            # Historical\n",
        "            {'name': 'Taj Mahal', 'location': 'Agra', 'state': 'UP', 'category': 'historical',\n",
        "             'description': 'Iconic monument', 'rating': 4.9, 'estimated_time_hours': 3.0,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 250},\n",
        "            {'name': 'Amber Fort', 'location': 'Jaipur', 'state': 'Rajasthan', 'category': 'historical',\n",
        "             'description': 'Hill fort', 'rating': 4.7, 'estimated_time_hours': 3.5,\n",
        "             'best_time_of_day': 'afternoon', 'entry_fee': 200},\n",
        "            {'name': 'Hampi Ruins', 'location': 'Hampi', 'state': 'Karnataka', 'category': 'historical',\n",
        "             'description': 'UNESCO site', 'rating': 4.8, 'estimated_time_hours': 4.0,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 40},\n",
        "            {'name': 'Red Fort', 'location': 'Delhi', 'state': 'Delhi', 'category': 'historical',\n",
        "             'description': 'Mughal fort', 'rating': 4.6, 'estimated_time_hours': 2.5,\n",
        "             'best_time_of_day': 'afternoon', 'entry_fee': 35},\n",
        "\n",
        "            # Nature\n",
        "            {'name': 'Valley of Flowers', 'location': 'Chamoli', 'state': 'Uttarakhand', 'category': 'nature',\n",
        "             'description': 'Alpine sanctuary', 'rating': 4.8, 'estimated_time_hours': 5.0,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 150},\n",
        "            {'name': 'Kerala Backwaters', 'location': 'Alleppey', 'state': 'Kerala', 'category': 'nature',\n",
        "             'description': 'Serene waterways', 'rating': 4.7, 'estimated_time_hours': 5.0,\n",
        "             'best_time_of_day': 'afternoon', 'entry_fee': 0},\n",
        "            {'name': 'Jim Corbett Park', 'location': 'Nainital', 'state': 'Uttarakhand', 'category': 'nature',\n",
        "             'description': 'Tiger reserve', 'rating': 4.7, 'estimated_time_hours': 4.0,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 600},\n",
        "\n",
        "            # Adventure\n",
        "            {'name': 'Rishikesh Rafting', 'location': 'Rishikesh', 'state': 'Uttarakhand', 'category': 'adventure',\n",
        "             'description': 'River rafting', 'rating': 4.8, 'estimated_time_hours': 4.0,\n",
        "             'best_time_of_day': 'morning', 'entry_fee': 500},\n",
        "            {'name': 'Bir Billing', 'location': 'Bir', 'state': 'HP', 'category': 'adventure',\n",
        "             'description': 'Paragliding', 'rating': 4.7, 'estimated_time_hours': 3.0,\n",
        "             'best_time_of_day': 'afternoon', 'entry_fee': 2500},\n",
        "\n",
        "            # Cultural\n",
        "            {'name': 'City Palace Jaipur', 'location': 'Jaipur', 'state': 'Rajasthan', 'category': 'cultural',\n",
        "             'description': 'Royal palace', 'rating': 4.6, 'estimated_time_hours': 2.5,\n",
        "             'best_time_of_day': 'afternoon', 'entry_fee': 200},\n",
        "\n",
        "            # Food\n",
        "            {'name': 'Old Delhi Food Walk', 'location': 'Delhi', 'state': 'Delhi', 'category': 'food',\n",
        "             'description': 'Street food tour', 'rating': 4.8, 'estimated_time_hours': 3.0,\n",
        "             'best_time_of_day': 'evening', 'entry_fee': 500},\n",
        "        ]\n",
        "\n",
        "        for data in attractions_data:\n",
        "            attraction = Attraction(**data)\n",
        "            db.session.add(attraction)\n",
        "\n",
        "        db.session.commit()\n",
        "        print(f\"‚úÖ Seeded {len(attractions_data)} attractions!\")\n",
        "\n",
        "print(\"\\n‚úÖ Database ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNMye8GuHwJX",
        "outputId": "d5f229bf-ecb5-49c9-d31e-54b059339c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Cleaning up...\n",
            "üöÄ Starting NLTK-enhanced server...\n",
            "\n",
            "======================================================================\n",
            "‚úÖ NLTK-POWERED TRAVEL PLANNER IS LIVE!\n",
            "======================================================================\n",
            "\n",
            "üéØ NLP FEATURES ENABLED:\n",
            "   ‚úì Tokenization (NLTK)\n",
            "   ‚úì POS Tagging - Nouns, Verbs, Adjectives\n",
            "   ‚úì Lemmatization (WordNet)\n",
            "   ‚úì Stopword Removal\n",
            "   ‚úì Sentiment Analysis (TextBlob)\n",
            "   ‚úì Keyword Extraction\n",
            "\n",
            "üåç SUPPORTED LANGUAGES:\n",
            "   English, Hindi, Bengali, Tamil, Telugu, Malayalam,\n",
            "   Marathi, Gujarati, Kannada, Punjabi, Odia, Assamese, Urdu\n",
            "\n",
            "üìã COPY THIS URL:\n",
            "   https://thymy-nonincarnate-jax.ngrok-free.dev\n",
            "======================================================================\n",
            "\n",
            "üß™ Test endpoints:\n",
            "   ‚Ä¢ https://thymy-nonincarnate-jax.ngrok-free.dev/\n",
            "   ‚Ä¢ https://thymy-nonincarnate-jax.ngrok-free.dev/api/languages\n",
            "   ‚Ä¢ https://thymy-nonincarnate-jax.ngrok-free.dev/api/attractions\n",
            "\n",
            "üí° Watch console for detailed NLP logs!\n",
            "\n",
            "‚ö†Ô∏è  Keep this cell running!\n",
            "======================================================================\n",
            "\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:16:23] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:16:47] \"OPTIONS / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:16:48] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:16:56] \"OPTIONS /api/users HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:16:57] \"\u001b[35m\u001b[1mPOST /api/users HTTP/1.1\u001b[0m\" 201 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:17:03] \"OPTIONS /api/groups HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:17:04] \"\u001b[35m\u001b[1mPOST /api/groups HTTP/1.1\u001b[0m\" 201 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:17:32] \"OPTIONS /api/preferences HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üîç NLTK-POWERED NLP PROCESSING\n",
            "======================================================================\n",
            "üåê Language detected: mr - Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)\n",
            "üîÑ Translation:\n",
            "   Original (mr): ‡§Ü‡§Æ‡•ç‡§π‡•Ä ‡§ï‡•Å‡§ü‡•Å‡§Ç‡§¨‡§æ‡§∏‡§π ‡§¨‡•ç‡§∞‡§æ‡§ù‡§ø‡§≤‡§≤‡§æ ‡§ú‡§æ‡§ä ‡§á‡§ö‡•ç‡§õ‡§ø‡§§‡•ã. ‡•´ ‡§¶‡§ø‡§µ‡§∏‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡§ï‡§ø‡§®‡§æ‡§∞‡•á, ‡§â‡§¶‡•ç‡§Ø‡§æ‡§®‡•á ‡§Ü‡§£‡§ø ‡§Æ‡•Å‡§≤‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® ‡§π‡§µ‡•á ‡§Ü‡§π‡•á. ‡§Ü‡§Æ‡§ö‡•á ‡§¨‡§ú‡•á‡§ü ‡•´‡•¶‡•¶‡•¶‡•¶ ‡§∞‡•Å‡§™‡§Ø‡•á ‡§Ü‡§π‡•á. ‡§Ü‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§®‡§ø‡§∏‡§∞‡•ç‡§ó ‡§Ü‡§£‡§ø ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§†‡§ø‡§ï‡§æ‡§£‡•á ‡§™‡§æ‡§π‡§æ‡§Ø‡§ö‡•Ä ‡§Ü‡§π‡•á‡§§.\n",
            "   English: We want to go to Brazil with family. 5 days worth of beaches, parks and entertainment for kids. Our budget is Rs.50000. We want to see beautiful nature and safe places.\n",
            "üìù NLTK Tokens (first 15): ['we', 'want', 'to', 'go', 'to', 'brazil', 'with', 'family', '.', '5', 'days', 'worth', 'of', 'beaches', ',']\n",
            "üßπ Stopword removal: 36 ‚Üí 18 tokens\n",
            "üî§ Lemmatized (first 10): ['want', 'go', 'brazil', 'family', '5', 'day', 'worth', 'beach', 'park', 'entertainment']\n",
            "üè∑Ô∏è POS Analysis:\n",
            "   Nouns: ['Brazil', 'family', 'days', 'beaches', 'parks', 'entertainment', 'kids', 'budget']\n",
            "   Verbs: ['want', 'go', 'is', 'want', 'see']\n",
            "   Adjectives: ['beautiful', 'safe']\n",
            "üòä Sentiment: Positive üòä (score: 0.55)\n",
            "üè∑Ô∏è Categories extracted: ['nature', 'beach']\n",
            "üí∞ Budget detected (keyword): low\n",
            "üìÖ Duration extracted: 5 days\n",
            "üîë Keywords extracted: ['rs.50000', 'nature', 'want', 'entertainment', 'family', 'parks', 'beautiful', 'days', 'places', 'brazil', 'safe', 'budget', 'kids', 'beaches']\n",
            "======================================================================\n",
            "‚úÖ NLP PROCESSING COMPLETE\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:17:37] \"\u001b[35m\u001b[1mPOST /api/preferences HTTP/1.1\u001b[0m\" 201 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:18:15] \"OPTIONS /api/groups/1/itinerary HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:18:15] \"\u001b[35m\u001b[1mPOST /api/groups/1/itinerary HTTP/1.1\u001b[0m\" 201 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:23:55] \"OPTIONS /api/users HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:23:55] \"POST /api/users HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:24:13] \"OPTIONS /api/preferences HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üîç NLTK-POWERED NLP PROCESSING\n",
            "======================================================================\n",
            "üåê Language detected: ta - Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:24:14] \"\u001b[35m\u001b[1mPOST /api/preferences HTTP/1.1\u001b[0m\" 201 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Translation:\n",
            "   Original (ta): ‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æ§‡Øç‡Æ§‡Æ∞‡Æï‡Ææ‡Æ£‡Øç‡Æü‡Æø‡Æ≤‡Øç 4 ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆØ‡Øã‡Æï‡Ææ ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡ÆÜ‡Æ©‡Øç‡ÆÆ‡ØÄ‡Æï ‡Æ™‡ÆØ‡Æ£‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡Æµ‡Æø‡Æ∞‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç. ‡Æ∞‡Æø‡Æ∑‡Æø‡Æï‡Øá‡Æ∑‡Æø‡Æ≤‡Øç ‡ÆØ‡Øã‡Æï‡Ææ ‡Æµ‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øç, ‡ÆÜ‡Æö‡Æø‡Æ∞‡ÆÆ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æï‡Æô‡Øç‡Æï‡Øà ‡ÆÜ‡Æ±‡Øç‡Æ±‡Æô‡Øç‡Æï‡Æ∞‡Øà ‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç. ‡Æé‡Æ©‡Øç ‡Æ™‡Æü‡Øç‡Æú‡ØÜ‡Æü‡Øç 30000 ‡Æ∞‡ØÇ‡Æ™‡Ææ‡ÆØ‡Øç.\n",
            "   English: I want to do yoga and spiritual journey in Uttarakhand for 4 days. Yoga classes, ashrams and the banks of the Ganges are must-sees in Rishikesh. My budget is 30000 rupees.\n",
            "üìù NLTK Tokens (first 15): ['i', 'want', 'to', 'do', 'yoga', 'and', 'spiritual', 'journey', 'in', 'uttarakhand', 'for', '4', 'days', '.', 'yoga']\n",
            "üßπ Stopword removal: 35 ‚Üí 16 tokens\n",
            "üî§ Lemmatized (first 10): ['want', 'yoga', 'spiritual', 'journey', 'uttarakhand', '4', 'day', 'yoga', 'class', 'ashram']\n",
            "üè∑Ô∏è POS Analysis:\n",
            "   Nouns: ['yoga', 'journey', 'Uttarakhand', 'days', 'Yoga', 'classes', 'ashrams', 'banks']\n",
            "   Verbs: ['want', 'do', 'are', 'is']\n",
            "   Adjectives: ['spiritual']\n",
            "üòä Sentiment: Neutral üòê (score: 0.00)\n",
            "üè∑Ô∏è Categories extracted: ['religious']\n",
            "üí∞ Budget detected (keyword): low\n",
            "üìÖ Duration extracted: 4 days\n",
            "üîë Keywords extracted: ['uttarakhand', 'spiritual', 'rupees', 'must-sees', 'want', 'banks', 'yoga', 'days', 'budget', 'classes', 'rishikesh', 'ganges', 'journey', 'ashrams']\n",
            "======================================================================\n",
            "‚úÖ NLP PROCESSING COMPLETE\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:24:41] \"OPTIONS /api/groups/1/itinerary HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:24:41] \"\u001b[35m\u001b[1mPOST /api/groups/1/itinerary HTTP/1.1\u001b[0m\" 201 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:27:30] \"OPTIONS / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:27:30] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:27:42] \"OPTIONS /api/users HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:27:42] \"\u001b[35m\u001b[1mPOST /api/users HTTP/1.1\u001b[0m\" 201 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:28:00] \"OPTIONS /api/groups HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:28:01] \"\u001b[35m\u001b[1mPOST /api/groups HTTP/1.1\u001b[0m\" 201 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:28:23] \"OPTIONS /api/preferences HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üîç NLTK-POWERED NLP PROCESSING\n",
            "======================================================================\n",
            "üåê Language detected: mr - Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:28:24] \"\u001b[35m\u001b[1mPOST /api/preferences HTTP/1.1\u001b[0m\" 201 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Translation:\n",
            "   Original (mr): ‡§Ü‡§Æ‡•ç‡§π‡•Ä ‡§π‡§®‡•Ä‡§Æ‡•Ç‡§®‡§∏‡§æ‡§†‡•Ä ‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§ñ‡§Ç‡§°‡§≤‡§æ ‡§ú‡§æ‡§ä ‡§á‡§ö‡•ç‡§õ‡§ø‡§§‡•ã. ‡•™ ‡§¶‡§ø‡§µ‡§∏‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§∞‡•ã‡§Æ‡§Å‡§ü‡§ø‡§ï ‡§π‡§ø‡§≤ ‡§∏‡•ç‡§ü‡•á‡§∂‡§®‡•ç‡§∏, ‡§∂‡§æ‡§Ç‡§§ ‡§∏‡§∞‡•ã‡§µ‡§∞‡•á ‡§Ü‡§£‡§ø ‡§≤‡§ï‡•ç‡§ù‡§∞‡•Ä ‡§∞‡§ø‡§∏‡•â‡§∞‡•ç‡§ü‡•ç‡§∏ ‡§π‡§µ‡•á ‡§Ü‡§π‡•á‡§§. ‡§Ü‡§Æ‡§ö‡•á ‡§¨‡§ú‡•á‡§ü ‡•≠‡•¶‡•¶‡•¶‡•¶ ‡§∞‡•Å‡§™‡§Ø‡•á ‡§Ü‡§π‡•á. ‡§®‡•à‡§®‡•Ä‡§§‡§æ‡§≤ ‡§Ü‡§£‡§ø ‡§Æ‡§∏‡•Ç‡§∞‡•Ä‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§®‡§ø‡§∏‡§∞‡•ç‡§ó ‡§Ö‡§®‡•Å‡§≠‡§µ‡§æ‡§Ø‡§ö‡§æ ‡§Ü‡§π‡•á.\n",
            "   English: We want to go to Uttarakhand for honeymoon. Romantic hill stations, serene lakes and luxury resorts are all you need for 4 days. Our budget is Rs.70000. Beautiful nature is to be experienced in Nainital and Mussoorie.\n",
            "üìù NLTK Tokens (first 15): ['we', 'want', 'to', 'go', 'to', 'uttarakhand', 'for', 'honeymoon', '.', 'romantic', 'hill', 'stations', ',', 'serene', 'lakes']\n",
            "üßπ Stopword removal: 42 ‚Üí 20 tokens\n",
            "üî§ Lemmatized (first 10): ['want', 'go', 'uttarakhand', 'honeymoon', 'romantic', 'hill', 'station', 'serene', 'lake', 'luxury']\n",
            "üè∑Ô∏è POS Analysis:\n",
            "   Nouns: ['Uttarakhand', 'honeymoon', 'Romantic', 'hill', 'stations', 'lakes', 'luxury', 'resorts']\n",
            "   Verbs: ['want', 'go', 'are', 'need', 'is']\n",
            "   Adjectives: ['serene']\n",
            "üòä Sentiment: Positive üòä (score: 0.55)\n",
            "üè∑Ô∏è Categories extracted: ['nature']\n",
            "üí∞ Budget detected (keyword): low\n",
            "üìÖ Duration extracted: 4 days\n",
            "üîë Keywords extracted: ['rs.70000', 'honeymoon', 'romantic', 'need', 'nature', 'nainital', 'mussoorie', 'stations', 'budget', 'resorts', 'uttarakhand', 'lakes', 'luxury', 'hill', 'serene']\n",
            "======================================================================\n",
            "‚úÖ NLP PROCESSING COMPLETE\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:28:58] \"OPTIONS /api/groups/2/itinerary HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Nov/2025 06:28:58] \"\u001b[35m\u001b[1mPOST /api/groups/2/itinerary HTTP/1.1\u001b[0m\" 201 -\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token\n",
        "ngrok.set_auth_token(\"35o5ALOM4iWSujVlrjCWcW4BVfc_2dUYJxYdbNeSq5GQuZXTy\")\n",
        "\n",
        "# Clean up\n",
        "print(\"üîÑ Cleaning up...\")\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    time.sleep(2)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Start tunnel\n",
        "try:\n",
        "    print(\"üöÄ Starting NLTK-enhanced server...\")\n",
        "\n",
        "    public_url = ngrok.connect(5000)\n",
        "\n",
        "    # Extract clean URL\n",
        "    url_str = str(public_url)\n",
        "    if '\"' in url_str:\n",
        "        clean_url = url_str.split('\"')[1]\n",
        "    elif \"->\" in url_str:\n",
        "        clean_url = url_str.split(\"->\")[0].strip()\n",
        "    else:\n",
        "        clean_url = url_str\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"‚úÖ NLTK-POWERED TRAVEL PLANNER IS LIVE!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nüéØ NLP FEATURES ENABLED:\")\n",
        "    print(f\"   ‚úì Tokenization (NLTK)\")\n",
        "    print(f\"   ‚úì POS Tagging - Nouns, Verbs, Adjectives\")\n",
        "    print(f\"   ‚úì Lemmatization (WordNet)\")\n",
        "    print(f\"   ‚úì Stopword Removal\")\n",
        "    print(f\"   ‚úì Sentiment Analysis (TextBlob)\")\n",
        "    print(f\"   ‚úì Keyword Extraction\")\n",
        "    print(f\"\\nüåç SUPPORTED LANGUAGES:\")\n",
        "    print(f\"   English, Hindi, Bengali, Tamil, Telugu, Malayalam,\")\n",
        "    print(f\"   Marathi, Gujarati, Kannada, Punjabi, Odia, Assamese, Urdu\")\n",
        "    print(f\"\\nüìã COPY THIS URL:\")\n",
        "    print(f\"   {clean_url}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nüß™ Test endpoints:\")\n",
        "    print(f\"   ‚Ä¢ {clean_url}/\")\n",
        "    print(f\"   ‚Ä¢ {clean_url}/api/languages\")\n",
        "    print(f\"   ‚Ä¢ {clean_url}/api/attractions\")\n",
        "    print(f\"\\nüí° Watch console for detailed NLP logs!\")\n",
        "    print(f\"\\n‚ö†Ô∏è  Keep this cell running!\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üîß TROUBLESHOOTING:\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n1. Go to: https://dashboard.ngrok.com/tunnels/agents\")\n",
        "    print(\"2. Stop all tunnels\")\n",
        "    print(\"3. Run this cell again\")\n",
        "    print(\"\\nOR Runtime ‚Üí Restart runtime and run all cells\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ybDGizFlNU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example 1: English\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 1: English Processing\")\n",
        "print(\"=\"*50)\n",
        "test_en = \"I want to visit beautiful temples and amazing beaches for 5 days with a budget of 20000\"\n",
        "lang = lang_processor.detect_language(test_en)\n",
        "tokens = lang_processor.tokenize_text(test_en)\n",
        "filtered = lang_processor.remove_stopwords(tokens)\n",
        "lemmatized = lang_processor.lemmatize_tokens(filtered)\n",
        "pos = lang_processor.extract_pos_tags(test_en)\n",
        "sentiment = lang_processor.analyze_sentiment(test_en)\n",
        "categories = pref_extractor.extract_categories(test_en, lemmatized)\n",
        "budget = pref_extractor.extract_budget(test_en)\n",
        "duration = pref_extractor.extract_duration(test_en)\n",
        "\n",
        "print(f\"\\n‚úÖ Results:\")\n",
        "print(f\"Language: {lang}\")\n",
        "print(f\"Tokens: {tokens[:10]}\")\n",
        "print(f\"Lemmatized: {lemmatized[:10]}\")\n",
        "print(f\"Nouns: {pos['nouns'][:5]}\")\n",
        "print(f\"Verbs: {pos['verbs'][:5]}\")\n",
        "print(f\"Sentiment: {sentiment}\")\n",
        "print(f\"Categories: {categories}\")\n",
        "print(f\"Budget: {budget}\")\n",
        "print(f\"Duration: {duration}\")\n",
        "\n",
        "# Example 2: Hindi\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 2: Hindi Processing\")\n",
        "print(\"=\"*50)\n",
        "test_hi = \"‡§Æ‡•à‡§Ç 5 ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§î‡§∞ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§§‡§ü ‡§¶‡•á‡§ñ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç\"\n",
        "lang_hi = lang_processor.detect_language(test_hi)\n",
        "translated = lang_processor.translate_to_english(test_hi, lang_hi)\n",
        "print(f\"\\n‚úÖ Translation successful!\")\n",
        "print(f\"Original: {test_hi}\")\n",
        "print(f\"Translated: {translated}\")\n",
        "\n",
        "# Example 3: Tamil\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 3: Tamil Processing\")\n",
        "print(\"=\"*50)\n",
        "test_ta = \"‡Æ®‡Ææ‡Æ©‡Øç 5 ‡Æ®‡Ææ‡Æü‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æï‡Øã‡Æµ‡Æø‡Æ≤‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æï‡Æü‡Æ±‡Øç‡Æï‡Æ∞‡Øà‡Æï‡Æ≥‡Øà ‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï ‡Æµ‡Æø‡Æ∞‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç\"\n",
        "lang_ta = lang_processor.detect_language(test_ta)\n",
        "translated_ta = lang_processor.translate_to_english(test_ta, lang_ta)\n",
        "print(f\"\\n‚úÖ Translation successful!\")\n",
        "print(f\"Original: {test_ta}\")\n",
        "print(f\"Translated: {translated_ta}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ ALL TESTS PASSED!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}